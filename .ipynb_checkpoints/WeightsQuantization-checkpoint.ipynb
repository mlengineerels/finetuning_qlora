{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0069a496",
   "metadata": {},
   "source": [
    "### Weights Quantization\n",
    "\n",
    "#### Reducing the size of a Large Language Model using 8 - bit Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the req and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16163ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes>=0.39.0\n",
    "!pip install -q git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edae039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b35c2",
   "metadata": {},
   "source": [
    "def zeropoint_quantize(X):\n",
    "    # Calculate value range (denominator)\n",
    "    x_range = torch.max(X) - torch.min(X)\n",
    "    x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "    # Calculate scale\n",
    "    scale = 255 / x_range\n",
    "\n",
    "    # Shift by zero-point\n",
    "    zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "\n",
    "    # Scale and round the inputs\n",
    "    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eeb58",
   "metadata": {},
   "source": [
    "##### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daad793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, _ = zeropoint_quantize(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dec30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b07a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Flatten weight tensors\n",
    "weights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\n",
    "weights_abs = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])\n",
    "weights_zp = np.concatenate([t.cpu().numpy().flatten() for t in weights_zp])\n",
    "\n",
    "# Set background style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n",
    "\n",
    "# Plot the histograms for original and zero-point weights\n",
    "axs[0].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\n",
    "axs[0].hist(weights_abs, bins=150, alpha=0.5, label='Absmax weights', color='red', range=(-2, 2))\n",
    "\n",
    "# Plot the histograms for original and absmax weights\n",
    "axs[1].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\n",
    "axs[1].hist(weights_zp, bins=150, alpha=0.5, label='Zero-point weights', color='green', range=(-2, 2))\n",
    "\n",
    "# Add grid\n",
    "for ax in axs:\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add legend\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "\n",
    "# Add title and labels\n",
    "axs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\n",
    "axs[1].set_title('Comparison of Original and Zeropoint Quantized Weights', fontsize=16)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Weights', fontsize=14)\n",
    "    ax.set_ylabel('Count', fontsize=14)\n",
    "    ax.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n",
    "\n",
    "# Improve font\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e612d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679477c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation\n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, absmax_text)\n",
    "\n",
    "print(f\"Original perplexity: {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:   {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a804b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_8bit=True,\n",
    "                                             )\n",
    "print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228da5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e74f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Flatten weight tensors\n",
    "weights_int8 = [param.data.clone() for param in model_int8.parameters()]\n",
    "weights_int8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])\n",
    "\n",
    "# Set background style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10,5), dpi=300)\n",
    "\n",
    "# Plot the histograms\n",
    "ax.hist(weights, bins=150, alpha=0.5, label='Original weights',\n",
    "        color='blue', range=(-2, 2))\n",
    "ax.hist(weights_int8, bins=150, alpha=0.5, label='LLM.int8() weights',\n",
    "        color='red', range=(-2, 2))\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Comparison of Original and Dequantized Weights', fontsize=16)\n",
    "ax.set_xlabel('Weights', fontsize=14)\n",
    "ax.set_ylabel('Count', fontsize=14)\n",
    "plt.gca().yaxis.set_major_formatter(ticker.EngFormatter())\n",
    "\n",
    "# Improve font\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0541e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with quantized model\n",
    "text_int8 = generate_text(model_int8, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM.int8() model:\\n{text_int8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d9831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efd8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity (original):   {ppl.item():.2f}\")\n",
    "\n",
    "ppl = calculate_perplexity(model_int8, text_int8)\n",
    "print(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
